{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE - 464: Machine Learning Endsemester Assignment\n",
    "### Satyam Saxena - Chemical Engineering - 17045120\n",
    "***\n",
    "In this assignment, I have used a Three Layer Multilayer Perceptron for identifying signs of Diabetic Retinopathy in Messidor Image Sets. All features represent either a detected lesion, a descriptive feature of a anatomical part or an image-level descriptor. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the CSV File Containing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/satyamsaxena/Downloads/csv_result-messidor_features.csv', 'r') as f:\n",
    "    dataset = list(csv.reader(f, delimiter=','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prints the headers of the data that we have. Note that first column 'id' is the serial number column and the last column 'Class' represents the true label of the images as 1: Having Diabetic Retinopathy or 0: Not having Diabetic Retinopathy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', 'Class']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we remove the first row (headers) and first column (id) to obtain the numerical data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.      ,  1.      , 22.      , ...,  0.100025,  1.      ,\n",
       "         0.      ],\n",
       "       [ 1.      ,  1.      , 24.      , ...,  0.144414,  0.      ,\n",
       "         0.      ],\n",
       "       [ 1.      ,  1.      , 62.      , ...,  0.128548,  0.      ,\n",
       "         1.      ],\n",
       "       ...,\n",
       "       [ 1.      ,  0.      , 49.      , ...,  0.129843,  0.      ,\n",
       "         0.      ],\n",
       "       [ 1.      ,  1.      , 39.      , ...,  0.10669 ,  1.      ,\n",
       "         1.      ],\n",
       "       [ 1.      ,  1.      ,  7.      , ...,  0.088957,  0.      ,\n",
       "         0.      ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.array(dataset[1:],dtype = np.float)\n",
    "dataset = np.delete(dataset,0,axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a dataset with 1151 unique records each having 20 features, one of which is the true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1151, 20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training and test datasets (70% ; 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806\n",
      "345\n"
     ]
    }
   ],
   "source": [
    "training_data_size = round(dataset.shape[0]*0.7)\n",
    "print(training_data_size)\n",
    "test_data_size = dataset.shape[0] - training_data_size\n",
    "print(test_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(dataset_copy)\n",
    "training_data, test_data = dataset_copy[0:training_data_size],dataset_copy[training_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806, 20)\n",
      "(345, 20)\n"
     ]
    }
   ],
   "source": [
    "print(training_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing true image labels from the training data matrix and saving it as training outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_features, training_set_outputs = training_data[:,0:training_data.shape[1]-1],training_data[:,training_data.shape[1]-1]\n",
    "training_set_outputs = training_set_outputs.reshape(training_data.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 806)\n",
      "(1, 806)\n"
     ]
    }
   ],
   "source": [
    "training_set_features = training_set_features.transpose()\n",
    "training_set_outputs = training_set_outputs.transpose()\n",
    "print(training_set_features.shape)\n",
    "print(training_set_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing true image labels from the test data matrix and saving it as test outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_features, test_set_outputs = test_data[:,0:test_data.shape[1]-1],test_data[:,test_data.shape[1]-1]\n",
    "test_set_outputs = test_set_outputs.reshape(test_data.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 345)\n",
      "(1, 345)\n"
     ]
    }
   ],
   "source": [
    "test_set_features = test_set_features.transpose()\n",
    "test_set_outputs = test_set_outputs.transpose()\n",
    "print(test_set_features.shape)\n",
    "print(test_set_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Sizes Function\n",
    "#### This function returns the input layer size, the number of nodes in the hidden layer and the output layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, hyper):\n",
    "    \n",
    "    n_x = X.shape[0]\n",
    "    n_h = hyper[1]\n",
    "    n_y = Y.shape[0]\n",
    "\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising Parameters\n",
    "#### This function initialises the parameters W1, W2 (weights for input layer and hidden layer) as random values and b1, b2 as numpy arrays full of zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    #Creating a dictionary with all the initialised parameter values\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test example we can see that the parameters W1, W2, b1 and b2 have been initialised to their respective values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-1.16699607 -0.52503321 -1.83550517  0.26944362 -1.61231835  0.57064087\n",
      "  -0.90122079  0.70043787  0.3259946  -0.41656683]\n",
      " [-0.7586883   0.12014371  1.10724041 -0.084239    0.2160564   2.00095096\n",
      "  -0.568479    1.0855279   0.96576919 -0.6251339 ]\n",
      " [ 0.66241543  1.77041858  0.60407375 -1.28220531 -0.24560295 -0.85240378\n",
      "  -0.18780361  0.06416823  2.01903545  0.96145004]\n",
      " [ 0.92906555  0.60397873 -1.59721402  1.70806126  1.57312165 -0.26194329\n",
      "   0.38499459  0.84770345 -0.25127191 -0.37972622]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.70214433 -0.15717744  0.15714526  0.38816713]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "n_x, n_h, n_y = 10,4,1\n",
    "\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "   \n",
    "    # Retrieving each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.tanh(Z1) #Here I used the tanh function as the activation function for the first layer\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = sigmoid(Z2) #Here I used the sigmoid function (defined later) as the activation function for the second layer.\n",
    "    \n",
    "    # To ensure that dimensions of output probabilities are of the required shape\n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    # All the values of the calculated matrices are stored in the dictionary cache\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Cost Function\n",
    "#### This function calculates the average cost over all the training examples using the logistic regression cost function \n",
    "#### [ mean of -y(log A2) - (1-y) log (1-A2) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "    # Computing the cross-entropy cost   \n",
    "    logprobs = Y*np.log(A2) + (1-Y)*np.log(1-A2)\n",
    "    cost = -1*(1/m)*np.sum(logprobs)\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "#### This function propagates backwards through the neural network to compute the derivatives that will be used to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Retrieving W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    # Retrieving A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculating dW1, db1, dW2, db2. \n",
    "    dZ2 = A2-Y\n",
    "    dW2 = (1/m)*(np.dot(dZ2,A1.transpose()))\n",
    "    db2 = (1/m)*np.sum(dZ2,axis=1,keepdims = True)\n",
    "    dZ1 = (np.dot(W2.transpose(),dZ2))*(1-np.power(A1,2))\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.transpose())\n",
    "    db1 = (1/m)*np.sum(dZ1,axis = 1, keepdims = True)\n",
    "\n",
    "    #storing all derivatives in the grads dictionary\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Parameters\n",
    "#### This function computes the updated values of all the parameters using the derivatives computed in backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, hyper):\n",
    "    \n",
    "    # Retrieving each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    learning_rate = hyper[0]\n",
    "\n",
    "    # Retrieving each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Updating the parameters\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "\n",
    "    #storing updated parameters in the parameters dictionary\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "#### Using all the functions developed previous, now the actual training of neural network is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, hyperparameters):\n",
    "   \n",
    "    # Obtaining the Sizes of the Neural Network Layers\n",
    "    n_x = layer_sizes(X, Y,hyperparameters)[0]\n",
    "    n_h = layer_sizes(X, Y,hyperparameters)[1]\n",
    "    n_y = layer_sizes(X, Y,hyperparameters)[2]\n",
    "    \n",
    "    # Initialising parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Looping till convergence (epsilon taken as 3*10^-7 instead of 10^-5 for better performance\n",
    "    previous_cost = 0\n",
    "    converged = 0\n",
    "    iterations = 0\n",
    "    while(converged == 0):     \n",
    "        # Forward propagation\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function\n",
    "        cost = compute_cost(A2, Y)\n",
    "        delta = abs(cost - previous_cost)\n",
    "        if delta<0.0000003:\n",
    "            converged = 1\n",
    "        previous_cost = cost\n",
    "        # Backpropagation\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update\n",
    "        parameters = update_parameters(parameters, grads, hyperparameters)\n",
    "        iterations+=1\n",
    "                \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Parameters on One set of Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sig = 1/(1+np.exp(-1*z))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.11697758 -1.75150617  0.8536715   0.09766975  1.02176724 -0.37276105\n",
      "   0.55884165 -1.13461916  0.61999574 -1.02658354  2.30120287 -1.02413672\n",
      "  -0.92472023 -1.03420569 -0.18227466  2.14494273  0.69903177  0.16090665\n",
      "  -0.17848138]\n",
      " [-0.19941468  1.02005582  0.3117793  -0.81367823 -2.77143729  0.68936309\n",
      "  -0.43002483 -0.67211106 -0.0977886  -0.49294539  0.12787722 -1.66837468\n",
      "  -0.85668614 -1.38626657 -0.70937317  0.86284409 -1.35339935 -1.49438641\n",
      "  -0.72633391]\n",
      " [ 0.22338688  0.8615982   0.38537351 -0.11736551  0.72217736  0.10731715\n",
      "  -0.98236318  1.12859386 -0.18750083  0.90494197  0.17747108 -0.12581818\n",
      "   1.99410755  0.31430075  0.65551097 -0.10603993  1.04352308 -0.15928494\n",
      "   0.47958229]\n",
      " [-0.97592684  0.7688015   0.82258143 -0.38881001  2.58729928  1.57086437\n",
      "   0.99741213 -0.67635235 -0.79391917  0.10598948 -2.02180388 -0.7928525\n",
      "  -0.62125906 -0.31353704 -1.59269281 -0.60227407 -1.41389251 -0.70279065\n",
      "   0.42183299]\n",
      " [-1.80091901  1.39612743 -2.50142091 -2.1484848  -1.23660019  0.08389788\n",
      "   1.84491166  0.00794054  1.19456148  0.4941395  -0.59051239 -0.46052467\n",
      "  -1.23201738  0.66482935  1.10473001  0.28145159  0.41793096 -0.57240212\n",
      "   0.51339229]\n",
      " [-0.12684934 -1.67456073 -0.99739145  0.01859739 -1.16433556  0.629446\n",
      "  -0.46178987 -0.86049715  0.3721842  -2.0060841  -1.46562268  0.17544152\n",
      "  -1.33362303  0.69373117  0.48211323  0.2151302   0.58899672  0.19440705\n",
      "   0.18312478]\n",
      " [ 0.97824957 -0.2795466  -0.31715124 -0.43359362 -0.42416908  0.38723659\n",
      "   0.13458788  0.36356585 -0.20891245 -0.28817852 -1.07397141 -1.42874223\n",
      "  -0.09096788 -1.60211693 -0.89661144 -0.37052431  1.06142238 -0.42009948\n",
      "   0.16854531]\n",
      " [ 1.08215694 -1.82024245 -0.36734118 -0.30540749 -1.6220975   0.06703016\n",
      "   0.06870805 -0.39272221  1.20054663  0.33160075 -2.0983325  -1.2960508\n",
      "   0.22683069 -0.09476028  0.65049634 -0.8282629   0.59278343  0.37084225\n",
      "   0.30859649]]\n",
      "b1 = [[ 1.88235110e-06]\n",
      " [-4.17128382e-08]\n",
      " [ 1.18939463e-01]\n",
      " [ 7.96354025e-04]\n",
      " [ 4.63108702e-02]\n",
      " [-7.74333568e-03]\n",
      " [-1.30690751e-02]\n",
      " [-1.41705952e-02]]\n",
      "W2 = [[ 0.3341574   0.52017454 -0.72816645 -0.23782668 -0.60419453 -1.08307705\n",
      "   0.32185784  0.19559154]]\n",
      "b2 = [[0.13055457]]\n"
     ]
    }
   ],
   "source": [
    "found_parameters = nn_model(training_set_features,training_set_outputs,(0.1,8))\n",
    "print(\"W1 = \" + str(found_parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(found_parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(found_parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(found_parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "#### This function will allow us to predict the outputs given a set of computed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = A2\n",
    "    for i in range(predictions.shape[1]):\n",
    "        if predictions[0][i]>0.5:\n",
    "            predictions[0][i]=1\n",
    "        else:\n",
    "            predictions[0][i]=0    \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      "  1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      "  1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      "  1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      "  0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      "  0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n",
      "  1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      "  0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      "  1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      "  1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      "  1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0.\n",
      "  1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      "  1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      "  1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      "  1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      "  0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1.\n",
      "  1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      "  1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      "  0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1.\n",
      "  1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      "  1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      "  0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      "  1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      "  1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      "  1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      "  0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      "  1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
      "  1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      "  1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      "  1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "predicted_values = predict(found_parameters,training_set_features)\n",
    "print(predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the training accuracy for one set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.20347394540943 %\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in range(training_set_outputs.shape[1]):\n",
    "    if training_set_outputs[0][i]==predicted_values[0][i]:\n",
    "        counter+=1\n",
    "print(counter*100/training_set_outputs.shape[1],\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = [(0.01,4),(0.01,8),(0.05,4),(0.05,8),(0.1,4),(0.1,8),(0.4,4),(0.4,8),(0.8,4),(0.8,8)]\n",
    "hyperparameters[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running 2-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a 10 X 20 Numpy array that will hold the values of the cross-validation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_accuracies = np.zeros((10,20))\n",
    "cross_validation_accuracies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(k, data, data_outputs,hyper):\n",
    "    accuracies = []\n",
    "    #data: (19,806)\n",
    "    n = data.shape[1] #806\n",
    "    for i in range(k): #k=2\n",
    "        start = int((n*i)/k)\n",
    "        end = int((n*(i+1))/k-1) #402\n",
    "        validn_inputs = data[:,start:end+1]\n",
    "        validn_outputs = data_outputs[:,start:end+1]#[:,0->402]\n",
    "        training_inputs = np.append(data[:,0:start],data[:,end+1:n],axis=1)\n",
    "        training_outputs = np.append(data_outputs[:,0:start],data_outputs[:,end+1:n],axis=1) \n",
    "        found_parameters = nn_model(training_inputs,training_outputs,hyper)\n",
    "        predicted_values = predict(found_parameters,validn_inputs)\n",
    "        counter = 0\n",
    "        for j in range(predicted_values.shape[1]):\n",
    "            if validn_outputs[0][j]==predicted_values[0][j]:\n",
    "                counter+=1\n",
    "        accuracy = counter/predicted_values.shape[1]\n",
    "        accuracies.append(accuracy)\n",
    "    avg = sum(accuracies)/len(accuracies)\n",
    "    print(avg)\n",
    "    return (avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running K-Folds Cross Validation for 20 different random initialisation values for each of the ten sets of hyperparameters and storing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6315136476426799\n",
      "0.5955334987593053\n",
      "0.5471464019851117\n",
      "0.4975186104218362\n",
      "0.5421836228287841\n",
      "0.49379652605459057\n",
      "0.5521091811414391\n",
      "0.4987593052109181\n",
      "0.5111662531017369\n",
      "0.5620347394540943\n",
      "0.488833746898263\n",
      "0.5570719602977667\n",
      "0.543424317617866\n",
      "0.5186104218362283\n",
      "0.6600496277915633\n",
      "0.4975186104218362\n",
      "0.6104218362282878\n",
      "0.5297766749379652\n",
      "0.5186104218362283\n",
      "0.5223325062034739\n",
      "0.6401985111662531\n",
      "0.5942928039702233\n",
      "0.4925558312655087\n",
      "0.5223325062034739\n",
      "0.5062034739454094\n",
      "0.6761786600496278\n",
      "0.586848635235732\n",
      "0.6799007444168734\n",
      "0.6426799007444168\n",
      "0.5421836228287841\n",
      "0.5880893300248139\n",
      "0.6774193548387097\n",
      "0.5248138957816377\n",
      "0.6004962779156328\n",
      "0.6240694789081886\n",
      "0.6600496277915633\n",
      "0.5794044665012408\n",
      "0.5856079404466501\n",
      "0.6066997518610422\n",
      "0.5669975186104218\n",
      "0.6054590570719602\n",
      "0.5682382133995036\n",
      "0.5359801488833746\n",
      "0.6451612903225806\n",
      "0.5074441687344913\n",
      "0.48635235732009924\n",
      "0.4826302729528536\n",
      "0.5148883374689825\n",
      "0.532258064516129\n",
      "0.5372208436724566\n",
      "0.5682382133995036\n",
      "0.4838709677419355\n",
      "0.48263027295285355\n",
      "0.5223325062034739\n",
      "0.5024813895781638\n",
      "0.6240694789081886\n",
      "0.4789081885856079\n",
      "0.5310173697270472\n",
      "0.5930521091811414\n",
      "0.609181141439206\n",
      "0.5124069478908189\n",
      "0.598014888337469\n",
      "0.5397022332506203\n",
      "0.5359801488833746\n",
      "0.6178660049627791\n",
      "0.5930521091811414\n",
      "0.5049627791563276\n",
      "0.6811414392059554\n",
      "0.6612903225806451\n",
      "0.5459057071960298\n",
      "0.4913151364764268\n",
      "0.533498759305211\n",
      "0.6377171215880894\n",
      "0.5372208436724566\n",
      "0.6910669975186104\n",
      "0.5186104218362282\n",
      "0.5372208436724566\n",
      "0.609181141439206\n",
      "0.5124069478908189\n",
      "0.7121588089330024\n",
      "0.5781637717121588\n",
      "0.49255583126550867\n",
      "0.5669975186104218\n",
      "0.5049627791563276\n",
      "0.5893300248138957\n",
      "0.56575682382134\n",
      "0.6042183622828784\n",
      "0.6538461538461537\n",
      "0.5186104218362283\n",
      "0.467741935483871\n",
      "0.6116625310173698\n",
      "0.48759305210918114\n",
      "0.5111662531017369\n",
      "0.533498759305211\n",
      "0.49379652605459057\n",
      "0.5620347394540943\n",
      "0.48883374689826303\n",
      "0.5905707196029777\n",
      "0.5124069478908189\n",
      "0.531017369727047\n",
      "0.554590570719603\n",
      "0.598014888337469\n",
      "0.4975186104218362\n",
      "0.5384615384615384\n",
      "0.5707196029776674\n",
      "0.5570719602977667\n",
      "0.532258064516129\n",
      "0.4950372208436724\n",
      "0.694789081885856\n",
      "0.6774193548387097\n",
      "0.630272952853598\n",
      "0.6029776674937966\n",
      "0.5818858560794045\n",
      "0.6265508684863523\n",
      "0.6513647642679901\n",
      "0.5843672456575683\n",
      "0.5235732009925558\n",
      "0.641439205955335\n",
      "0.5992555831265509\n",
      "0.5893300248138957\n",
      "0.6290322580645161\n",
      "0.5099255583126551\n",
      "0.48883374689826303\n",
      "0.47766749379652607\n",
      "0.5818858560794045\n",
      "0.5037220843672456\n",
      "0.575682382133995\n",
      "0.532258064516129\n",
      "0.5210918114143921\n",
      "0.4801488833746898\n",
      "0.5607940446650124\n",
      "0.6327543424317619\n",
      "0.5471464019851117\n",
      "0.5024813895781637\n",
      "0.532258064516129\n",
      "0.6240694789081886\n",
      "0.501240694789082\n",
      "0.5037220843672456\n",
      "0.49503722084367247\n",
      "0.4987593052109181\n",
      "0.641439205955335\n",
      "0.533498759305211\n",
      "0.48883374689826303\n",
      "0.5409429280397022\n",
      "0.5359801488833746\n",
      "0.5794044665012408\n",
      "0.5037220843672456\n",
      "0.5310173697270472\n",
      "0.5037220843672456\n",
      "0.5347394540942928\n",
      "0.56575682382134\n",
      "0.533498759305211\n",
      "0.5409429280397022\n",
      "0.6153846153846154\n",
      "0.5384615384615384\n",
      "0.5384615384615384\n",
      "0.5210918114143921\n",
      "0.5942928039702233\n",
      "0.5918114143920595\n",
      "0.4950372208436724\n",
      "0.533498759305211\n",
      "0.5086848635235732\n",
      "0.49379652605459057\n",
      "0.6104218362282878\n",
      "0.5161290322580645\n",
      "0.533498759305211\n",
      "0.5397022332506204\n",
      "0.5421836228287842\n",
      "0.5186104218362282\n",
      "0.4652605459057072\n",
      "0.5570719602977667\n",
      "0.5124069478908189\n",
      "0.4950372208436724\n",
      "0.5471464019851117\n",
      "0.56575682382134\n",
      "0.5297766749379653\n",
      "0.5397022332506204\n",
      "0.5744416873449132\n",
      "0.49503722084367247\n",
      "0.5297766749379653\n",
      "0.5049627791563276\n",
      "0.48511166253101734\n",
      "0.5223325062034739\n",
      "0.5397022332506203\n",
      "0.49379652605459057\n",
      "0.4813895781637717\n",
      "0.56575682382134\n",
      "0.4838709677419355\n",
      "0.5\n",
      "0.5124069478908189\n",
      "0.5111662531017369\n",
      "0.5297766749379653\n",
      "0.5285359801488834\n",
      "0.5570719602977667\n",
      "0.5223325062034739\n",
      "0.488833746898263\n",
      "0.48759305210918114\n",
      "0.4640198511166253\n",
      "0.47022332506203474\n",
      "0.5074441687344913\n",
      "[[0.63151365 0.5955335  0.5471464  0.49751861 0.54218362 0.49379653\n",
      "  0.55210918 0.49875931 0.51116625 0.56203474 0.48883375 0.55707196\n",
      "  0.54342432 0.51861042 0.66004963 0.49751861 0.61042184 0.52977667\n",
      "  0.51861042 0.52233251]\n",
      " [0.64019851 0.5942928  0.49255583 0.52233251 0.50620347 0.67617866\n",
      "  0.58684864 0.67990074 0.6426799  0.54218362 0.58808933 0.67741935\n",
      "  0.5248139  0.60049628 0.62406948 0.66004963 0.57940447 0.58560794\n",
      "  0.60669975 0.56699752]\n",
      " [0.60545906 0.56823821 0.53598015 0.64516129 0.50744417 0.48635236\n",
      "  0.48263027 0.51488834 0.53225806 0.53722084 0.56823821 0.48387097\n",
      "  0.48263027 0.52233251 0.50248139 0.62406948 0.47890819 0.53101737\n",
      "  0.59305211 0.60918114]\n",
      " [0.51240695 0.59801489 0.53970223 0.53598015 0.617866   0.59305211\n",
      "  0.50496278 0.68114144 0.66129032 0.54590571 0.49131514 0.53349876\n",
      "  0.63771712 0.53722084 0.691067   0.51861042 0.53722084 0.60918114\n",
      "  0.51240695 0.71215881]\n",
      " [0.57816377 0.49255583 0.56699752 0.50496278 0.58933002 0.56575682\n",
      "  0.60421836 0.65384615 0.51861042 0.46774194 0.61166253 0.48759305\n",
      "  0.51116625 0.53349876 0.49379653 0.56203474 0.48883375 0.59057072\n",
      "  0.51240695 0.53101737]\n",
      " [0.55459057 0.59801489 0.49751861 0.53846154 0.5707196  0.55707196\n",
      "  0.53225806 0.49503722 0.69478908 0.67741935 0.63027295 0.60297767\n",
      "  0.58188586 0.62655087 0.65136476 0.58436725 0.5235732  0.64143921\n",
      "  0.59925558 0.58933002]\n",
      " [0.62903226 0.50992556 0.48883375 0.47766749 0.58188586 0.50372208\n",
      "  0.57568238 0.53225806 0.52109181 0.48014888 0.56079404 0.63275434\n",
      "  0.5471464  0.50248139 0.53225806 0.62406948 0.50124069 0.50372208\n",
      "  0.49503722 0.49875931]\n",
      " [0.64143921 0.53349876 0.48883375 0.54094293 0.53598015 0.57940447\n",
      "  0.50372208 0.53101737 0.50372208 0.53473945 0.56575682 0.53349876\n",
      "  0.54094293 0.61538462 0.53846154 0.53846154 0.52109181 0.5942928\n",
      "  0.59181141 0.49503722]\n",
      " [0.53349876 0.50868486 0.49379653 0.61042184 0.51612903 0.53349876\n",
      "  0.53970223 0.54218362 0.51861042 0.46526055 0.55707196 0.51240695\n",
      "  0.49503722 0.5471464  0.56575682 0.52977667 0.53970223 0.57444169\n",
      "  0.49503722 0.52977667]\n",
      " [0.50496278 0.48511166 0.52233251 0.53970223 0.49379653 0.48138958\n",
      "  0.56575682 0.48387097 0.5        0.51240695 0.51116625 0.52977667\n",
      "  0.52853598 0.55707196 0.52233251 0.48883375 0.48759305 0.46401985\n",
      "  0.47022333 0.50744417]]\n",
      "Time taken:  741.8662309646606\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(len(hyperparameters)):\n",
    "    for j in range(20):\n",
    "        cross_validation_accuracies[i][j] = k_fold_cross_validation(2,training_set_features,training_set_outputs,hyperparameters[i])\n",
    "\n",
    "print(cross_validation_accuracies)\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Time taken: \",toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Mean Cross-Validation Accuracy for each set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5439206 , 0.59485112, 0.54057072, 0.57853598, 0.54323821,\n",
       "       0.58734491, 0.53492556, 0.54640199, 0.53039702, 0.50781638])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_accuracies_mean = np.sum(cross_validation_accuracies,axis = 1)/20\n",
    "cross_validation_accuracies_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing the Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11d2c3e10>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(cross_validation_accuracies.transpose(), columns=['(0.01,4)','(0.01,8)','(0.05,4)','(0.05,8)','(0.1,4)','(0.1,8)','(0.4,4)','(0.4,8)','(0.8,4)','(0.8,8)'])\n",
    "df.plot.box(grid='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the best set of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the 25th percentile value and 75 percentile value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50806452 0.56079404 0.49844913 0.52977667 0.50217122 0.55055831\n",
      " 0.50062035 0.52853598 0.51147643 0.4869727 ]\n",
      "[0.55831266 0.64081886 0.57444169 0.62282878 0.58095533 0.62748139\n",
      " 0.56451613 0.56916873 0.54342432 0.52388337]\n"
     ]
    }
   ],
   "source": [
    "cross_validation_accuracies_25p = np.percentile(cross_validation_accuracies,25,axis =1)\n",
    "cross_validation_accuracies_75p = np.percentile(cross_validation_accuracies,75,axis =1)\n",
    "print(cross_validation_accuracies_25p)\n",
    "print(cross_validation_accuracies_75p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the interquartile range as a measure of variability of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05024814 0.08002481 0.07599256 0.09305211 0.07878412 0.07692308\n",
      " 0.06389578 0.04063275 0.03194789 0.03691067]\n"
     ]
    }
   ],
   "source": [
    "inter_quartile_range = cross_validation_accuracies_75p - cross_validation_accuracies_25p\n",
    "print(inter_quartile_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the inverse of interquartile range as a measure of consistency of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19.90123457, 12.49612403, 13.15918367, 10.74666667, 12.69291339,\n",
       "       13.        , 15.65048544, 24.61068702, 31.30097087, 27.09243697])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reciproc_iqr = 1/inter_quartile_range\n",
    "reciproc_iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the scores for performance of each set of hyperparameters by giving more weightage to the mean cross-validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.01, 4), (0.01, 8), (0.05, 4), (0.05, 8), (0.1, 4), (0.1, 8), (0.4, 4), (0.4, 8), (0.8, 4), (0.8, 8)]\n",
      "[563.8218301  607.34724066 553.72990328 589.28264682 555.93112679\n",
      " 600.34491315 550.57604375 571.01267213 561.69799321 534.90881415]\n"
     ]
    }
   ],
   "source": [
    "scores = reciproc_iqr + 1000*cross_validation_accuracies_mean #100 for converting to percentage and 10 as a weight\n",
    "print(hyperparameters)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607.3472406563178"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the maximum score value, we choose (0.01,8) as the best set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_hyper = (0.01,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Neural Network 20 times with random initialisations to compute mean Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_dash = []\n",
    "for j in range(20):\n",
    "    test_found_parameters = nn_model(training_set_features,training_set_outputs,chosen_hyper)\n",
    "    predicted_values = predict(test_found_parameters,test_set_features)\n",
    "    counter = 0\n",
    "    for i in range(test_set_outputs.shape[1]):\n",
    "        if test_set_outputs[0][i]==predicted_values[0][i]:\n",
    "            counter+=1\n",
    "    accuracy = (counter*100/test_set_outputs.shape[1])\n",
    "    accuracies_dash.append(accuracy)\n",
    "mean_test_accuracy_dash = sum(accuracies_dash)/len(accuracies_dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy on Test Data = 65.71 %\n",
      "Maximum Accuracy on Test Data = 75.65 %\n",
      "Minimum Accuracy on Test Data = 50.14 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Accuracy on Test Data = {a:2.2f} %\".format(a=mean_test_accuracy_dash))\n",
    "print(\"Maximum Accuracy on Test Data = {b:2.2f} %\".format(b=max(accuracies_dash)))\n",
    "print(\"Minimum Accuracy on Test Data = {c:2.2f} %\".format(c=min(accuracies_dash)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Match Characteristic Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies_values = [mean_test_accuracy_dash,100]\n",
    "ranks = ['1','2']\n",
    "plt.plot(ranks,accuracies_values,'r-')\n",
    "plt.ylim(0,100)\n",
    "plt.xlim(0,2)\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
